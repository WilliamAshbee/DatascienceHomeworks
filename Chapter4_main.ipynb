{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter4_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilliamAshbee/DatascienceHomeworks/blob/main/Chapter4_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16beFe8589Sd"
      },
      "source": [
        "# Data Preprocessing in Python\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_Jw5TyH-wPI"
      },
      "source": [
        "First start with importing modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLBC9rYE8_tm"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFA4TPeP-39i"
      },
      "source": [
        "**Reading Data from CSV file**\n",
        "Then, read the Motor Insurance Fraud Claim data to a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2bxyEvV5q6V"
      },
      "source": [
        "# this is only needed for this google colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# abt_path = '/content/drive/My Drive/data/MotorInsuranceFraudClaimABTFull.csv'\n",
        "\n",
        "# for your local jupyter notebook, simply use the abs. or rel. path for the file\n",
        "# comment out above code.\n",
        "abt_path = './MotorInsuranceFraudClaimABTFull.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvAfxwvxBwpT"
      },
      "source": [
        "Now read the csv file using the `pd`. `df` is the variable name of the data frame. `df.head()` shows the first few records (default is 5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BqbrJYj64BL"
      },
      "source": [
        "df = pd.read_csv(abt_path, sep=',')\n",
        "# you can alternatively use index_col parameter when reading csv files.\n",
        "# df = pd.read_csv(abt_path, sep=',', index_col='ID')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ua4iBoXCVgB"
      },
      "source": [
        "`df.dtypes` would return all the column names and their data types (int64, float64, or object).\n",
        "`df.info()` provides a more complex view with number of non-null values, data types etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mmt-bdnCVqb"
      },
      "source": [
        "# df.dtypes\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB5VHBUxEt6s"
      },
      "source": [
        "**Iterating over Data Frames**\n",
        "\n",
        "While it is not advisible, sometimes you may eventually need to iterate through records in your data frame. You can use `df.iterrows()`, `df.itertuples()`, or `df.iteritems()` to iterate. `iterrows()` returns a Series object per each row, and generally the slowest. `itertuples()` and `iteritems()` lets you iterate over row and column series as named tuples. We can use a `for` loop for iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sEP1TBUEuEP"
      },
      "source": [
        "for index, row in df.iterrows():\n",
        "    print(index, end=' ')\n",
        "  # print(row) # uncomment this to see"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK-9KV3fKpdV"
      },
      "source": [
        "Checking the basics of your data. For every column in the data frame, we can count the records (`df[name].size`), count NULL values (`sum(df[name].isnull().sum())`), and unique values (`df[name].unique().size`). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKGZgoarKplU"
      },
      "source": [
        "# We will use df.iteritems() as it allows us to iterate through columns\n",
        "for (name, series) in df.iteritems():\n",
        "  print('ANALYZING THE COLUMN:', name)\n",
        "  print('\\tTotal number of records', series.size)\n",
        "  print('\\tNumber of missing values', series.isnull().sum())\n",
        "  print('\\tPercentage of missing values {0}%'.format(((series.isnull().sum()/series.size)*100)) )\n",
        "  print('\\tNumber of unique values', series.unique().size)\n",
        "\n",
        "#alternatively, you can iterate over df.columns (column names) and access the columns\n",
        "# for name in df.columns:\n",
        "#   print('ANALYZING THE COLUMN:', name)\n",
        "#   print('\\tTotal number of records', df[name].size)\n",
        "#   print('\\tNumber of missing values', df[name].isnull().sum())\n",
        "#   print('\\tPercentage of missing values {0}%'.format(((df[name].isnull().sum()/df[name].size)*100)) )\n",
        "#   print('\\tNumber of unique values', df[name].unique().size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4JSM1WiQE1r"
      },
      "source": [
        "**Handling Missing Values**\n",
        "\n",
        "For a larger DataFrame, it can be tedious to look for missing values manually. In this case, we can use the `isnull()` method to return a DataFrame with Boolean values that indicate whether a cell contains a numeric value (False) or if data is missing (True)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQKip3OuQFAh"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E7WN1aRQal2"
      },
      "source": [
        "Rows with missing values can be easily dropped via the `dropna()` method. Please, note significant data reduction – we went from 7,000 cells to 2,338 cells. `df.size` returns *(row-count x column-count)* when called for the entire DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ4csXksQatj"
      },
      "source": [
        "print(df.isnull().sum())\n",
        "print('Number of cells before dropping rows with missing values: {}'.format(df.size))\n",
        "print()\n",
        "dfrd = df.dropna() # drop values with null values and re-assign the copy\n",
        "print(dfrd.isnull().sum())\n",
        "print('Number of cells after dropping rows with missing values: {}'.format(dfrd.size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TyuWTkfRySD"
      },
      "source": [
        "Instead of dropping rows, we can also drop columns with null values by setting the `axis` argument to 1, *i.e.*, `df = df.dropna(axis=1)`. This time we will use `df.shape` for feedback, as it returns separate counts for both dimensions of DataFrame, that is the (row-count, column-count) pair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjLLY2UWSJBI"
      },
      "source": [
        "print(df.isnull().sum())\n",
        "print('Number of cells before dropping columns with missing values: {}'.format(df.size))\n",
        "print()\n",
        "dfcd = df.dropna(axis=1) # drop columns with row values (axis=1 signifies columns)\n",
        "print(dfcd.isnull().sum())\n",
        "print('Number of cells after dropping columns with missing values: {}'.format(dfcd.size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giKN5pfHUMdc"
      },
      "source": [
        "**Imputation:**\n",
        "We can also replace each NaN value by the corresponding mean which is separately calculated for each feature column. Other options for this strategy are replacing with median or most frequent, which is useful for categorical feature values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2XL-e76U9DA"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputed_df = df.copy()\n",
        "imputed_df['Num Soft Tissue'] = mean_imputer.fit_transform(df[['Num Soft Tissue']])\n",
        "# Note here that we cannot use mean for Marital Status, simply because it is not a numeric attribute\n",
        "print('\\tTotal number of missing values in original df', df.isnull().sum().sum())\n",
        "print('\\tTotal number of missing values after imputation (for number of soft tissue)', imputed_df.isnull().sum().sum())\n",
        "\n",
        "#There is a much more straightforward way for mean imputation but above can be used with much more flexible options\n",
        "#df.apply(lambda x: x.fillna(x.mean()),axis=0) # this assumes all numerical values and uses lambda functions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM-nSSyqdA0t"
      },
      "source": [
        "**Histograms**\n",
        "\n",
        "We used `df.dtype` and `df.info()` before. We will be iterating through the entire dataframe again, but only selecting the attributes that are different than object. \n",
        "\n",
        "We import a popular plotting library `matplotlib` to generate\n",
        "histograms and then save each of them in a separate PDF file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smakTc_hdUJ3"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqIEKf_udfoU"
      },
      "source": [
        "col_list = []\n",
        "for (name, series) in df.iteritems():\n",
        "    if series.dtype != 'object':\n",
        "        col_list.append(name)\n",
        "\n",
        "fig, ax = plt.subplots(2,5,figsize=(18,8)) # get a bigger figure\n",
        "df_for_hist = df[col_list]\n",
        "\n",
        "df_for_hist.hist(bins=10, alpha=0.5, ax=ax) \n",
        "#note here that you may change the bin size to see the distribution better.\n",
        "# df_for_hist.hist(bins=20, alpha=0.5, ax=ax)\n",
        "#you could also do the following\n",
        "# print(df.select_dtypes(exclude=['object']).hist(bins=6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKpUYUpugiaW"
      },
      "source": [
        "**Box Plots**\n",
        "\n",
        "A box plot displays the five-number summary of a set of data. The five-number summary is the minimum, first quartile, median, third quartile, and maximum.\n",
        "In a box plot, we draw a box from the first quartile to the third quartile. A vertical line goes through the box at the median. The whiskers go from each quartile to the minimum or maximum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_zyyeevP3Af"
      },
      "source": [
        "# Let's get the numeric data again, with another handy function\n",
        "df_for_bp = df._get_numeric_data()\n",
        "\n",
        "#you can plot one by one\n",
        "plt.boxplot(df_for_bp['ID'])\n",
        "plt.show()\n",
        "\n",
        "# or you can use pandas-provided wrapper.\n",
        "# df_for_bp.boxplot(column=['Total Claimed', 'Num Claims'])\n",
        "df_for_bp.boxplot(column=['ID'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt1UP2DjSu-h"
      },
      "source": [
        "The above-presented box-plots for ID column shows a nicely spread values, very similar to a normal distribution. However, we cannot infer that right away."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWKTP2LoS20I"
      },
      "source": [
        "# Let's consider claim amount\n",
        "df_for_bp.boxplot(column=['Claim Amount'], vert=False)\n",
        "\n",
        "# Let's also see the values used in the five-number summary\n",
        "print('CLAIM AMOUNT')\n",
        "print('\\tMin value:{}'.format(df_for_bp['Claim Amount'].min())) # minimum\n",
        "print('\\t1st Quartile value:{}'.format(df_for_bp['Claim Amount'].quantile(0.25))) # First quartile\n",
        "print('\\tMedian value:{}'.format(df_for_bp['Claim Amount'].median())) # median\n",
        "print('\\t3rd Quartile value:{}'.format(df_for_bp['Claim Amount'].quantile(0.75))) # Third quartile\n",
        "print('\\tMax value:{}'.format(df_for_bp['Claim Amount'].max())) # maximum\n",
        "\n",
        "# additional descriptive statistics\n",
        "print('\\tMean value:{}'.format(df_for_bp['Claim Amount'].mean())) # mean\n",
        "print('\\tStd. Dev. value:{}'.format(df_for_bp['Claim Amount'].std())) # std dev\n",
        "print('\\tVariance value:{}'.format(df_for_bp['Claim Amount'].var())) # variance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py4_TqdZVBI0"
      },
      "source": [
        "**Scatter Plots**\n",
        "\n",
        "A scatter plot is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlFdBXO3VNGL"
      },
      "source": [
        "df_scatter = df._get_numeric_data()\n",
        "\n",
        "#let's use classical method\n",
        "plt.scatter(x=df_scatter['Income of Policy Holder'], y=df_scatter['Claim Amount'])\n",
        "plt.xlabel('Income of Policy Holder')\n",
        "plt.ylabel('Claim Amount')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWWhqcChVzEN"
      },
      "source": [
        "# we can also use pandas wrapper\n",
        "df_scatter.plot(kind='scatter', x='Income of Policy Holder', y='Claim Amount', c='teal', s=50, marker='X')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jiAqyv-p2CT"
      },
      "source": [
        "# seaborn also has a scatterplot function, which provides neat functionality\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(data=df, x=\"Income of Policy Holder\", y=\"Claim Amount\", \n",
        "                hue=\"Num Claimants\", style=\"Injury Type\", size=\"Num Claimants\", alpha=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiRl7jCop2CU"
      },
      "source": [
        "# you can also use 3D scatters when needed.  \n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# plot\n",
        "fig = plt.figure(figsize=(8, 6), dpi=100)\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter3D(xs=df_scatter['Income of Policy Holder'], ys=df_scatter['Claim Amount'], \n",
        "             zs=df_scatter['Num Claimants'], c=df_scatter['Num Claimants'], s=20)\n",
        "ax.set_xlabel('Income')\n",
        "ax.set_ylabel('Claim Amount')\n",
        "ax.set_zlabel('# Claimants')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rifUYGqJWGJJ"
      },
      "source": [
        "# We can also create a scatter plot matrix (SPLOM). \n",
        "# Seaborn, which is a visualization library, provides a high-level interface for these.\n",
        "import seaborn as sns\n",
        "\n",
        "df_concise = df_scatter[ ['Income of Policy Holder', 'Claim Amount', 'Claim Amount Received', 'Fraud Flag'] ]\n",
        "\n",
        "# sns.set(style='ticks')\n",
        "sns.pairplot(df_concise, hue='Fraud Flag', markers=['+', 'o']) # hue is for mapping plot aspects to different colors.\n",
        "\n",
        "# you can use the following to see the full pairplot\n",
        "# sns.pairplot(df_scatter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9uylbuLYC8f"
      },
      "source": [
        "**Heatmap of Correlation Matrix**\n",
        "\n",
        "A heat map is a graphical representation of data where the individual values contained in a matrix are represented as colors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJCl7rfGYMyy"
      },
      "source": [
        "correlations = df_concise.corr()\n",
        "sns.heatmap(correlations)\n",
        "print(correlations.to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvRZ5rGeY-ld"
      },
      "source": [
        "**Min-max Normalization**\n",
        "is the simplest method and consists in rescaling the range of features to scale the range in [0, 1] or [−1, 1]. Selecting the target range depends on the nature of the data. The general formula for a min-max of [0, 1] is given as:\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOmzTKcwZL0F"
      },
      "source": [
        "# we will use scikit-learn library for normalization\n",
        "from sklearn import preprocessing\n",
        "\n",
        "df_minmax = df_concise # for quicker illustration\n",
        "print(df_minmax.head())\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "a_scaled = min_max_scaler.fit_transform(df_minmax) #a_scaled is an N-d numpy array now. We need to create a new dataframe off of it\n",
        "df_scaled = pd.DataFrame(data=a_scaled, columns=df_minmax.columns)\n",
        "df_scaled.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhwqjtyEjaPS"
      },
      "source": [
        "**Z-score Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edQzK6qWjdlr"
      },
      "source": [
        "zs_scaler = preprocessing.StandardScaler()\n",
        "zs_scaled = zs_scaler.fit_transform(df_minmax)\n",
        "df_zscaled = pd.DataFrame(data=zs_scaled, columns=df_minmax.columns)\n",
        "df_zscaled.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNN_0HY7p2CW"
      },
      "source": [
        "There are many more normalization techniques available for use in scikit-learn. You can find a nice comparison [in this tutorial](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84Fo55aqku_G"
      },
      "source": [
        "**Equal-width Binning**\n",
        "\n",
        "We can use the histogram function of `numpy`:\n",
        "\n",
        "`count,division = np.histogram(series)`\n",
        "\n",
        "where `division` is the automatically calculated border for your bins and `count` is the population inside each bin. We have equal-width bins setup as [min, max] range with linearly spaced 5 markers (i.e., this results in 4 bin between the markers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc0qnqkgkuIr"
      },
      "source": [
        "count, division = np.histogram(df['Claim Amount'], bins=5)\n",
        "division"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}